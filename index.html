<!doctype html>
<html><head><title data-react-helmet="true">Casey Chu</title><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"/><link data-react-helmet="true" rel="stylesheet" href="/static/style.css"/><link data-react-helmet="true" rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/></head><body id="index"><div class="index"><header class="header-out"><a href="/" class="avatar"></a><div class="header-info"><div class="header-name">Casey Chu</div><div class="header-email"><a href="mailto:caseychu9@gmail.com">caseychu9@gmail.com</a></div></div></header><header class="index-header"><span class="avatar"></span><div class="index-header-info"><div class="index-header-name">Casey Chu</div><div class="index-header-email"><a href="mailto:caseychu9@gmail.com" target="_blank">caseychu9@gmail.com</a></div></div><div class="index-header-description"><p>Hello! I’m a researcher at <a href="https://openai.com/">OpenAI</a>, working on multimodal AI systems. Let’s chat sometime!</p><p>Previously, I was a PhD student in <a href="https://icme.stanford.edu">computational math at Stanford University</a> (although I left early), and a math major at <a href="https://www.hmc.edu/">Harvey Mudd College</a>. Find me on <a href="https://twitter.com/caseychu9" rel="me"><i class="fa fa-twitter"></i> Twitter</a>, <a href="https://github.com/caseychu" rel="me"><i class="fa fa-github"></i> GitHub</a>, or <a href="http://stackoverflow.com/users/298233/casey" rel="me"><i class="fa fa-stack-overflow"></i> Stack Overflow</a>.</p></div></header><section class="section" id="publications"><h1 class="section-title">Research</h1><div class="story story-publication "><a href="https://arxiv.org/abs/2303.08774" class="story-img" style="background-image:url(/static/content/gpt-4.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2303.08774" class="story-title" target="_blank">GPT-4</a><div class="story-date">2023</div><div class="story-authors"><i>Led the development of the initial prototype for visual inputs</i></div><div class="story-venue"></div><div class="story-extra"><a href="https://arxiv.org/abs/2303.08774" target="_blank">paper</a> &middot; <a href="https://openai.com/research/gpt-4" target="_blank">announcement</a></div><div class="story-blurb">GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/2204.06125" class="story-img" style="background-image:url(/static/content/dalle-2.jpg)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2204.06125" class="story-title" target="_blank">DALL&middot;E 2: Hierarchical Text-Conditional Image Generation<br />with CLIP Latents</a><div class="story-date">2022</div><div class="story-authors">Aditya Ramesh*, Prafulla Dhariwal*, Alex Nichol*, <b>Casey Chu</b>*, Mark Chen</div><div class="story-venue"></div><div class="story-extra"><a href="https://arxiv.org/abs/2204.06125" target="_blank">paper</a> &middot; <a href="https://openai.com/product/dall-e-2" target="_blank">announcement</a></div><div class="story-blurb">DALL&middot;E 2 can create original, realistic images and art from a text description. It is capable of combining concepts, attributes, and styles. We propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/2004.01822" class="story-img" style="background-image:url(/static/content/svgd.jpg)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2004.01822" class="story-title" target="_blank">The equivalence between Stein variational gradient descent and black-box variational inference</a><div class="story-date">2020</div><div class="story-authors"><b>Casey Chu</b>, Kentaro Minami, Kenji Fukumizu</div><div class="story-venue">ICLR 2020 <a href="http://iclr2020deepdiffeq.rice.edu/" target="_blank">DeepDiffEq Workshop</a></div><div class="story-extra"><a href="https://arxiv.org/abs/2004.01822" target="_blank">paper</a> &middot; <a href="https://www.youtube.com/watch?v=56Jh16gMKI8" target="_blank">video</a></div><div class="story-blurb">We formalize an equivalence between two popular methods for Bayesian inference: Stein variational gradient descent (SVGD) and black-box variational inference (BBVI). In particular, we show that BBVI corresponds precisely to SVGD when the kernel is the neural tangent kernel. Furthermore, we interpret SVGD and BBVI as kernel gradient flows; we do this by leveraging the recent perspective that views SVGD as a gradient flow in the space of probability distributions and showing that BBVI naturally motivates a Riemannian structure on that space. We observe that kernel gradient flow also describes dynamics found in the training of generative adversarial networks (GANs). This work thereby unifies several existing techniques in variational inference and generative modeling and identifies the kernel as a fundamental object governing the behavior of these algorithms, motivating deeper analysis of its properties.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/2002.04185" class="story-img" style="background-image:url(/static/content/smoothness.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/2002.04185" class="story-title" target="_blank">Smoothness and Stability in GANs</a><div class="story-date">2020</div><div class="story-authors"><b>Casey Chu</b>, Kentaro Minami, Kenji Fukumizu</div><div class="story-venue">ICLR 2020</div><div class="story-extra"><a href="https://arxiv.org/abs/2004.01822" target="_blank">paper</a> &middot; <a href="https://www.youtube.com/watch?v=5ZRL06xLeVQ" target="_blank">video</a></div><div class="story-blurb">Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/1901.10691" class="story-img" style="background-image:url(/static/content/pfd.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1901.10691" class="story-title" target="_blank">Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning</a><div class="story-date">2019</div><div class="story-authors"><b>Casey Chu</b>, Jose Blanchet, Peter Glynn</div><div class="story-venue">ICML 2019</div><div class="story-extra"><a href="https://arxiv.org/abs/1901.10691" target="_blank">paper</a> &middot; <a href="/static/content/pfd-poster.pdf" target="_blank">poster</a> &middot; <a href="/static/content/pfd-slides-short.pdf" target="_blank">slides</a> &middot; <a href="/static/content/pfd-slides-long.pdf" target="_blank">detailed slides</a></div><div class="story-blurb">This paper provides a unifying view of a wide range of problems of interest in machine learning by framing them as the minimization of functionals defined on the space of probability measures. In particular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic optimization algorithm for our formulation, called probability functional descent (PFD), and show how this algorithm recovers existing methods developed independently in the settings mentioned earlier.</div></div></div><div class="story story-publication "><a href="https://arxiv.org/abs/1712.02950" class="story-img" style="background-image:url(/static/content/cyclegan.jpg)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1712.02950" class="story-title" target="_blank">CycleGAN, a Master of Steganography</a><div class="story-date">2017</div><div class="story-authors"><b>Casey Chu</b>, Andrey Zhmoginov, Mark Sandler</div><div class="story-venue">NIPS 2017 <a href="https://www.machinedeception.com/" target="_blank">Workshop on Machine Deception</a></div><div class="story-extra"><a href="https://arxiv.org/abs/1712.02950" target="_blank">paper</a> &middot; <a href="/static/content/cyclegan-slides.pdf" target="_blank"> slides</a> &middot; <a href="https://techcrunch.com/2018/12/31/this-clever-ai-hid-data-from-its-creators-to-cheat-at-its-appointed-task/" target="_blank">press</a></div><div class="story-blurb">CycleGAN <a href="https://junyanz.github.io/CycleGAN/" target="_blank">(Zhu et al. 2017)</a> is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.</div></div></div></section><section class="section" id="posts"><h1 class="section-title">Blog Posts</h1><div class="story-basic"><a href="/posts/a-bayesian-trains-a-classifier/" class="story-title">A Bayesian trains a classifier</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/perspectives-on-the-variational-autoencoder/" class="story-title">Perspectives on the variational autoencoder</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/maximum-entropy-kl-divergence-and-bayesian-inference/" class="story-title">The principle of maximum entropy</a><div class="story-date">July 2018</div></div><div class="story-basic"><a href="/posts/flavors-of-wasserstein-gan/" class="story-title">Flavors of Wasserstein GAN</a><div class="story-date">March 2018</div></div><div class="story-basic"><a href="/posts/why-does-algebra-work/" class="story-title">Why does algebra work?</a><div class="story-date">January 2013</div></div><div class="story-basic"><a href="/posts/the-dirichlet-function-in-closed-form/" class="story-title">The Dirichlet function in closed form</a><div class="story-date">March 2012</div></div></section><section class="section" id="notes"><h1 class="section-title">Course Notes</h1><div class="story-basic"><a href="/notes/algebraic-topology/" class="story-title">Algebraic topology</a></div><div class="story-basic"><a href="/notes/algorithms/" class="story-title">Algorithms</a></div><div class="story-basic"><a href="/notes/numerical-linear-algebra/" class="story-title">Numerical linear algebra</a></div><div class="story-basic"><a href="/notes/numerical-partial-differential-equations/" class="story-title">Numerical partial differential equations</a></div><div class="story-basic"><a href="/notes/partial-differential-equations/" class="story-title">Partial differential equations</a></div><div class="story-basic"><a href="/notes/statistical-mechanics/" class="story-title">Statistical mechanics</a></div><div class="story-basic"><a href="/notes/reinforcement-learning/" class="story-title">Reinforcement learning</a></div></section></div></body></html>