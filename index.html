<!doctype html>
<html><head><title data-react-helmet="true">Casey Chu</title><link data-react-helmet="true" rel="stylesheet" href="/static/style.css"/><link data-react-helmet="true" rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"/></head><body id="index"><div><header class="index-header"><span class="avatar"></span><div class="index-header-info"><div class="index-header-name">Casey Chu</div><div class="index-header-email"><a href="mailto:caseychu@stanford.edu" target="_blank">caseychu@stanford.edu</a></div></div><div class="index-header-description"><p>Hello! Iâ€™m a Ph.D. student in <a href="https://icme.stanford.edu">computational math</a> at Stanford University, specializing in deep learning.</p><p>Previously, I majored in math at Harvey Mudd College, and I was an intern at Google and Facebook. Find me on <a href="http://stackoverflow.com/users/298233/casey" rel="me"><i class="fa fa-stack-overflow"></i> Stack Overflow</a> and <a href="https://github.com/caseychu" rel="me"><i class="fa fa-github"></i> GitHub</a>, or check out my <a href="/static/resume.pdf" rel="me">resume</a>.</p></div></header><section class="section" id="posts"><h1 class="section-title">Blog Posts</h1><div class="story-basic"><a href="/posts/a-bayesian-trains-a-classifier/" class="story-title">A Bayesian trains a classifier</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/perspectives-on-the-variational-autoencoder/" class="story-title">Perspectives on the variational autoencoder</a><div class="story-date">November 2018</div></div><div class="story-basic"><a href="/posts/maximum-entropy-kl-divergence-and-bayesian-inference/" class="story-title">Maximum entropy, KL divergence, and Bayesian inference</a><div class="story-date">July 2018</div></div><div class="story-basic"><a href="/posts/flavors-of-wasserstein-gan/" class="story-title">Flavors of Wasserstein GAN</a><div class="story-date">March 2018</div></div><div class="story-basic"><a href="/posts/why-does-algebra-work/" class="story-title">Why does algebra work?</a><div class="story-date">January 2013</div></div><div class="story-basic"><a href="/posts/the-dirichlet-function-in-closed-form/" class="story-title">The Dirichlet function in closed form</a><div class="story-date">March 2012</div></div></section><section class="section" id="publications"><h1 class="section-title">Publications</h1><div class="story story-publication "><a href="https://arxiv.org/abs/1712.02950" class="story-img" style="background-image:url(/static/content/cyclegan.png)" target="_blank"></a><div class="story-info"><a href="https://arxiv.org/abs/1712.02950" class="story-title" target="_blank">CycleGAN, a Master of Steganography</a><div class="story-date">2017</div><div class="story-authors"><b>Casey Chu</b>, Andrey Zhmoginov, Mark Sandler</div><div class="story-venue">NIPS 2017, <a href="https://www.machinedeception.com/" target="_blank">Workshop on Machine Deception</a></div><div class="story-blurb">CycleGAN <a href="https://junyanz.github.io/CycleGAN/" target="_blank">(Zhu et al. 2017)</a> is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to "hide" information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.</div></div></div></section><section class="section" id="notes"><h1 class="section-title">Course Notes</h1><div class="story-basic"><a href="/notes/algebraic-topology/" class="story-title">Algebraic topology</a></div><div class="story-basic"><a href="/notes/algorithms/" class="story-title">Algorithms</a></div><div class="story-basic"><a href="/notes/numerical-linear-algebra/" class="story-title">Numerical linear algebra</a></div><div class="story-basic"><a href="/notes/numerical-partial-differential-equations/" class="story-title">Numerical partial differential equations</a></div><div class="story-basic"><a href="/notes/partial-differential-equations/" class="story-title">Partial differential equations</a></div><div class="story-basic"><a href="/notes/statistical-mechanics/" class="story-title">Statistical mechanics</a></div><div class="story-basic"><a href="/notes/reinforcement-learning/" class="story-title">Reinforcement learning</a></div></section></div></body></html>